{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-25 16:20:54.872226: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import polars as pl\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import  Bidirectional, GRU, Dense, Dropout,LSTM\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(\"X_test_m4HAPAP.csv\")\n",
    "X_train = pd.read_csv(\"X_train_N1UvY30.csv\")\n",
    "y_train = pd.read_csv(\"y_train_or6m3Ta.csv\")\n",
    "num_classes = 24\n",
    "Y= to_categorical(y_train['eqt_code_cat'], num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables cat√©gorielles √† encoder\n",
    "categorical_columns = ['venue', 'action', 'side']\n",
    "\n",
    "# Variables num√©riques\n",
    "numeric_columns = ['price', 'bid', 'ask', 'bid_size', 'ask_size', 'flux']\n",
    "\n",
    "# Encoding\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    X_train[col] = le.fit_transform(X_train[col])\n",
    "    X_test[col] = le.transform(X_test[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature_engineering(df):\n",
    "    df['bid_ask_ratio'] = df['bid_size'] / df['ask_size']\n",
    "    df['Imbalance'] = df['bid_size'] - df['ask_size']\n",
    "\n",
    "    df['price_change'] = df.groupby('obs_id')['price'].diff()\n",
    "    df['cumulative_price_change'] = df.groupby('obs_id')['price_change'].cumsum()\n",
    "    df['bid_change'] = df.groupby('obs_id')['bid'].diff()\n",
    "    df['ask_change'] = df.groupby('obs_id')['ask'].diff()\n",
    "    # VWAP (Volume-Weighted Average Price)\n",
    "    # VWAP is calculated as the sum of (price * volume) divided by the total volume\n",
    "    df['vwap'] = df.groupby('obs_id').apply(lambda g: (g['price'] * g['bid_size']).sum() / g['bid_size'].sum()).reindex(df.index)\n",
    "\n",
    "    df['price_zscore'] = (df['price'] - df['price'].mean()) / df['price'].std()\n",
    "    df['trade_volume'] = df['flux'] * df['trade'].astype(int)\n",
    "\n",
    "     # 8. Application des transformations demand√©es\n",
    "    df['log_bid_size'] = np.log(np.abs(df['bid_size'] + 1))\n",
    "    df['log_ask_size'] = np.log(np.abs(df['ask_size'] + 1))\n",
    "    df['log_flux'] = np.log(np.abs(df['flux']) + 1) * np.sign(df['flux'])\n",
    "\n",
    "    return df\n",
    "\n",
    "X_train = add_feature_engineering(X_train)\n",
    "X_test = add_feature_engineering(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [ 'venue', 'action', 'trade', \n",
    "                    'price',\n",
    "                    'bid',\n",
    "                    'ask',\n",
    "                    'bid_size',\n",
    "                    'ask_size',\n",
    "                    'flux',\n",
    "                    'Imbalance',\n",
    "                    'bid_ask_ratio',\n",
    "                    'cumulative_price_change',\n",
    "                    'price_change',\n",
    "                    'bid_change',\n",
    "                    'ask_change',\n",
    "                    'price_zscore',\n",
    "                    'trade_volume',\n",
    "                    'log_bid_size',\n",
    "                    'log_ask_size',\n",
    "                    'log_flux'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_observation(X_train, selected_columns, sequence_length=100):\n",
    "    grouped = X_train.groupby('obs_id')\n",
    "\n",
    "    # Pr√©allocation d'un tableau pour stocker toutes les s√©quences\n",
    "    num_groups = len(grouped)\n",
    "    sequences = np.zeros((num_groups, sequence_length, len(selected_columns)))  # Shape: (num_groups, sequence_length, num_features)\n",
    "\n",
    "    for i, (_, group) in enumerate(grouped):\n",
    "        # Prendre les sequence_length premiers √©v√©nements si plus\n",
    "        sequence = group[selected_columns].values[:sequence_length]\n",
    "\n",
    "        # Si moins de sequence_length √©v√©nements, on applique du padding (remplissage avec des z√©ros)\n",
    "        if len(sequence) < sequence_length:\n",
    "            sequences[i, :len(sequence)] = sequence  # Remplir les lignes avec les donn√©es existantes\n",
    "            # Les lignes restantes sont d√©j√† initialis√©es √† z√©ro\n",
    "        else:\n",
    "            sequences[i] = sequence  # Remplacer par la s√©quence compl√®te\n",
    "\n",
    "    return sequences\n",
    "\n",
    "\n",
    "\n",
    "# Utilisation de la fonction\n",
    "X_train = group_by_observation(X_train, selected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# D√©finir le mod√®le\n",
    "class MyGRUModel(nn.Module):\n",
    "    def __init__(self, num_unique_venues, num_unique_actions,num_unique_trade, embedding_dim, gru_units, num_classes):\n",
    "        super(MyGRUModel, self).__init__()\n",
    "        self.venue_embedding = nn.Embedding(num_unique_venues, embedding_dim)\n",
    "        self.action_embedding = nn.Embedding(num_unique_actions, embedding_dim)\n",
    "        self.gru = nn.GRU(input_size=embedding_dim * 2 + 16, hidden_size=gru_units, batch_first=True, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(gru_units * 2, 64)  # 2 car c'est bidirectionnel\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        venue_emb = self.venue_embedding(x[:, :, 0].long())\n",
    "        action_emb = self.action_embedding(x[:, :, 1].long())\n",
    "        trade_emb = self.action_embedding(x[:, :, 2].long())\n",
    "        continuous_features = x[:, :, 3:]  # Assurez-vous que les dimensions correspondent\n",
    "        features = torch.cat((venue_emb, action_emb,trade_emb, continuous_features), dim=-1)\n",
    "\n",
    "        # Passer les donn√©es dans GRU\n",
    "        gru_out, _ = self.gru(features)\n",
    "\n",
    "        # On peut prendre la sortie de la derni√®re √©tape ou faire une autre op√©ration ici\n",
    "        output = self.fc1(gru_out[:, -1, :])\n",
    "        output = torch.nn.functional.selu(output)\n",
    "        output = self.fc2(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "print (\"class init done\")\n",
    "# Param√®tres\n",
    "embedding_dim = 8\n",
    "gru_units = 64\n",
    "num_classes = 24\n",
    "\n",
    "# Instancier le mod√®le\n",
    "model = MyGRUModel(num_unique_venues=6, num_unique_actions=3,num_unique_trade=2, embedding_dim=embedding_dim, \n",
    "                   gru_units=gru_units, num_classes=num_classes)\n",
    "\n",
    "# D√©placer le mod√®le vers le GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Pr√©parer vos donn√©es\n",
    "inputs = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "targets = torch.tensor(Y, dtype=torch.float32).to(device)\n",
    "\n",
    "# Cr√©er DataLoader\n",
    "dataset = TensorDataset(inputs, targets)\n",
    "dataloader = DataLoader(dataset, batch_size=1000, shuffle=True)\n",
    "\n",
    "# Configuration de l'optimiseur et de la fonction de perte\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"model configuration = OK \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_observations = X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#STANDARD PARAMETERS#\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = .... \n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=5e-3)\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "\n",
    "#LOSSES DEFINITIONS#\n",
    "\n",
    "#MCC LOSS \n",
    "\n",
    "\n",
    "\n",
    "class MCCLoss(nn.Module):\n",
    "    def __init__(self, T=20, epsilon=1e-5):\n",
    "        super(MCCLoss, self).__init__()\n",
    "        self.T = T\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def entropy(self, input):\n",
    "        \"\"\"Compute the entropy for each batch element\"\"\"\n",
    "        input = torch.clamp(input, min=self.epsilon)  # √âviter les log(0)\n",
    "        entropy = -input * torch.log(input)\n",
    "        return torch.sum(entropy, dim=1)\n",
    "\n",
    "    def forward(self, targets):\n",
    "        \"\"\"\n",
    "        Compute the MCC loss\n",
    "        - targets: tensor of shape (batch_size, nb_class) with the model's logits\n",
    "        \"\"\"\n",
    "        batch_size, nb_class = targets.size()\n",
    "\n",
    "        # Rescale predictions with temperature\n",
    "        rescaled_targets = F.log_softmax(targets / self.T, dim=1)\n",
    "\n",
    "        # V√©rifier si le rescaled_targets contient des NaN et les √©viter\n",
    "        if torch.isnan(rescaled_targets).any():\n",
    "            print(\"NaN detected in softmax rescaled_targets.\")\n",
    "            rescaled_targets = torch.nan_to_num(rescaled_targets, nan=0.0)  # Remplacer NaN par 0\n",
    "\n",
    "        # Compute entropy and calculate targets' weights\n",
    "        targets_weights = self.entropy(rescaled_targets).detach()\n",
    "        targets_weights = 1 + torch.exp(-targets_weights)\n",
    "        targets_weights = batch_size * targets_weights / (torch.sum(targets_weights) + self.epsilon)  # Ajout d'epsilon pour √©viter division par z√©ro\n",
    "\n",
    "        # Compute covariance matrix\n",
    "        cov_matrix = rescaled_targets.mul(targets_weights.view(-1, 1)).transpose(1, 0).mm(rescaled_targets)\n",
    "\n",
    "        # Normaliser la matrice de covariance\n",
    "        cov_matrix /= (torch.sum(cov_matrix, dim=1, keepdim=True) + self.epsilon)  # Ajout d'epsilon\n",
    "\n",
    "        # V√©rifier s'il y a des NaN dans la matrice de covariance et les remplacer\n",
    "        if torch.isnan(cov_matrix).any():\n",
    "            print(\"NaN detected in covariance matrix.\")\n",
    "            cov_matrix = torch.nan_to_num(cov_matrix, nan=0.0)\n",
    "\n",
    "        # Compute MCC Loss\n",
    "        loss = (torch.sum(cov_matrix) - torch.trace(cov_matrix)) / (nb_class + self.epsilon)  # Ajout d'epsilon dans la normalisation\n",
    "        return loss\n",
    "\n",
    "\n",
    "#LABEL SMOOTHING LOSS \n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        targets = targets.long()  # S'assurer que c'est bien un entier\n",
    "        num_classes = outputs.size(1)  # Nombre de classes\n",
    "\n",
    "        # Convertir en one-hot encoding\n",
    "        true_dist = torch.zeros_like(outputs).scatter_(1, targets, 1) \n",
    "\n",
    "        # Appliquer le lissage\n",
    "        true_dist = true_dist * (1 - self.smoothing) + self.smoothing / num_classes\n",
    "\n",
    "        # Calcul de la perte\n",
    "        log_probs = nn.functional.log_softmax(outputs, dim=-1)\n",
    "        loss = torch.mean(torch.sum(-true_dist * log_probs, dim=-1))\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#BIB:  Label Smoothing : Szegedy et al., \"Rethinking the Inception Architecture for Computer Vision\"\n",
    "\n",
    "\n",
    "\n",
    "#TRAINING PLOT#\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_progress(train_losses, val_losses, train_accuracies, val_accuracies, n=10):\n",
    "    length = len(train_losses)\n",
    "    epochs = range(1, length + 1)\n",
    "    \n",
    "    if length % n == 0:\n",
    "        fig = plt.gcf()  # Get the current figure\n",
    "        if fig is None:\n",
    "            fig = plt.figure(figsize=(10, 6))  \n",
    "\n",
    "        plt.clf()  # Clear the current figure\n",
    "        \n",
    "        # Loss Plot \n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(epochs, train_losses, label=\"Train Loss\", color=\"blue\", linestyle='-', marker='o')\n",
    "        plt.plot(epochs, val_losses, label=\"Validation Loss\", color=\"red\", linestyle='--', marker='x')\n",
    "        plt.xlabel('Epoch')\n",
    "        \n",
    "        if (max(train_losses)-min(train_losses))/(max(train_losses)+min(train_losses))>1/10:\n",
    "            plt.yscale('log')\n",
    "            plt.ylabel('Loss (Logscale)')\n",
    "        else:\n",
    "            plt.ylabel('Loss')\n",
    "        plt.title(f'Train vs Validation Loss, epoch = {length}')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Accuracy Plot\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(epochs, train_accuracies, label=\"Train Accuracy\", color=\"blue\", linestyle='-', marker='o')\n",
    "        plt.plot(epochs, val_accuracies, label=\"Validation Accuracy\", color=\"red\", linestyle='--', marker='x')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title(f'Train vs Validation Accuracy, epoch = {length}')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Afficher la figure\n",
    "        plt.tight_layout()\n",
    "        plt.draw()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# TRAINING #\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler: torch.optim.lr_scheduler._LRScheduler,\n",
    "    num_epochs: int = 60,\n",
    "    alpha: float = 0.1,\n",
    "    beta: float = 0.1,\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    test_every: int = 10,\n",
    "):\n",
    "    \"\"\"Entra√Æne le mod√®le avec Label Smoothing et MCC Loss.\"\"\"\n",
    "\n",
    "    # D√©finition des fonctions de perte\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    label_smoothing_loss_fn = LabelSmoothingLoss()\n",
    "    mcc_loss_fn = MCCLoss()\n",
    "\n",
    "    # Envoi du mod√®le sur l'appareil cible (CPU/GPU)\n",
    "    model.to(device)\n",
    "    print(f\"üîπ Training on device: {device}\")\n",
    "\n",
    "    best_val_accuracy = 0.0\n",
    "    best_model_state = None\n",
    "\n",
    "    # Suivi des m√©triques\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "\n",
    "    # Utilisation de `torch.cuda.amp` pour acc√©l√©rer l'entra√Ænement sur GPU\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # Boucle d'entra√Ænement\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        train_loss, train_accuracy, total_samples = 0.0, 0.0, 0\n",
    "        mcc_loss_accumulated = 0.0  # Suivi de la MCC Loss\n",
    "\n",
    "        for batch_idx, (inputs, targets) in enumerate(tqdm.tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\", leave=False)):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward avec `amp`\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Perte principale\n",
    "                loss =  criterion(outputs, targets.argmax(dim=1)) #+ alpha *label_smoothing_loss_fn(outputs, targets)\n",
    "\n",
    "                # MCC Loss (ajout√©e p√©riodiquement)\n",
    "                #if batch_idx % test_every == 0:\n",
    "                    # mcc_loss = mcc_loss_fn(outputs)\n",
    "                    # loss += beta * mcc_loss\n",
    "                    #mcc_loss_accumulated += mcc_loss.item()\n",
    "\n",
    "            # Backward avec `amp`\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            # Mise √† jour des m√©triques\n",
    "            print (loss.item)\n",
    "            train_loss += loss.item()\n",
    "            predicted = outputs.argmax(dim=1)\n",
    "            targets_accs = targets.argmax(dim=1)\n",
    "            train_accuracy += (predicted == targets_accs).sum().item()\n",
    "            total_samples += targets.size(0)\n",
    "\n",
    "        # Mise √† jour du scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # Moyenne des m√©triques\n",
    "        train_loss /= len(train_loader)\n",
    "        train_accuracy /= total_samples\n",
    "        mcc_loss_accumulated /= max(1, len(train_loader) // test_every)\n",
    "\n",
    "        print(f\"üìå Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, MCC Loss: {mcc_loss_accumulated:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_accuracy, val_samples = 0.0, 0.0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                loss = criterion(outputs, targets.argmax(dim=1)) #+ alpha *label_smoothing_loss_fn(outputs, targets)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                predicted = outputs.argmax(dim=1)\n",
    "                targets_accs = targets.argmax(dim=1)\n",
    "                val_accuracy += (predicted == targets_accs).sum().item()\n",
    "                val_samples += targets.size(0)\n",
    "\n",
    "                # MCC Loss (ajout√©e p√©riodiquement)\n",
    "                if batch_idx % test_every == 0:\n",
    "                    mcc_loss = mcc_loss_fn(outputs)\n",
    "                    val_loss += beta * mcc_loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy /= val_samples\n",
    "\n",
    "        print(f\"‚úÖ Validation: Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "        # Stockage des m√©triques\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        # Mise √† jour du meilleur mod√®le\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_model_state = model.state_dict()\n",
    "            \n",
    "        plot_training_progress(train_losses, val_losses, train_accuracies, val_accuracies, n=3)\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                print(f\"{name}: {param.grad.abs().mean():.6f}\")\n",
    "\n",
    "    # Chargement du meilleur mod√®le\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoques = (10_000 * 1000) // num_observations\n",
    "for epoch in range(epoques):  # Exemple avec 10 √©poques\n",
    "    model.train()  # Mettre le mod√®le en mode entra√Ænement\n",
    "    for batch_inputs, batch_targets in dataloader:\n",
    "        optimizer.zero_grad()  # R√©initialiser les gradients\n",
    "        outputs = model(batch_inputs)  # Passer les donn√©es dans le mod√®le\n",
    "        loss = criterion(outputs, batch_targets.argmax(dim=1))  # Calculer la perte\n",
    "        loss.backward()  # R√©tropropagation\n",
    "        optimizer.step()  # Mise √† jour des poids\n",
    "    print(f'Epoch [{epoch+1}/{str(epoques)}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = add_feature_engineering(X_test)\n",
    "X_test = group_by_observation(X_test, selected_columns)\n",
    "X_test_input = torch.tensor(X_test, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supposons que `test_dataset` contienne vos donn√©es de test\n",
    "test_loader = DataLoader(X_test_input, batch_size=256, shuffle=False)\n",
    "\n",
    "all_predictions = []\n",
    "all_probabilities = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_inputs in test_loader:  # Si vous avez besoin des cibles, sinon utilisez seulement batch_inputs\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "\n",
    "        # Faire la pr√©diction pour ce lot\n",
    "        outputs = model(batch_inputs)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        predicted_classes = torch.argmax(probabilities, dim=1)\n",
    "\n",
    "        # Stocker les r√©sultats\n",
    "        all_predictions.append(predicted_classes.cpu().numpy())\n",
    "        all_probabilities.append(probabilities.cpu().numpy())\n",
    "\n",
    "# Combiner tous les lots\n",
    "all_predictions = np.concatenate(all_predictions)\n",
    "all_probabilities = np.concatenate(all_probabilities)\n",
    "\n",
    "print(\"Pr√©dictions pour tout le dataset :\", all_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prediction = pd.DataFrame(data = {'eqt_code_cat':all_predictions})\n",
    "df_prediction\n",
    "df_prediction.to_csv('ypredic.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
